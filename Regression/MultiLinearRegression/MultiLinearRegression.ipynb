{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Oct 2023</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:34:13</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.855</td> <td>    7.281</td> <td> 0.000</td> <td> 3.63e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7542</td> <td> 3371.026</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.103</td> <td> 6992.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -42.0063</td> <td> 3256.058</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.161</td> <td> 6520.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.368</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.783</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.951   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.945   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     169.9   \\\\\n",
       "\\textbf{Date:}             & Tue, 10 Oct 2023 & \\textbf{  Prob (F-statistic):} &  1.34e-27   \\\\\n",
       "\\textbf{Time:}             &     20:34:13     & \\textbf{  Log-Likelihood:    } &   -525.38   \\\\\n",
       "\\textbf{No. Observations:} &          50      & \\textbf{  AIC:               } &     1063.   \\\\\n",
       "\\textbf{Df Residuals:}     &          44      & \\textbf{  BIC:               } &     1074.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &    5.013e+04  &     6884.855     &     7.281  &         0.000        &     3.63e+04    &      6.4e+04     \\\\\n",
       "\\textbf{x1}    &     198.7542  &     3371.026     &     0.059  &         0.953        &    -6595.103    &     6992.611     \\\\\n",
       "\\textbf{x2}    &     -42.0063  &     3256.058     &    -0.013  &         0.990        &    -6604.161    &     6520.148     \\\\\n",
       "\\textbf{x3}    &       0.8060  &        0.046     &    17.368  &         0.000        &        0.712    &        0.900     \\\\\n",
       "\\textbf{x4}    &      -0.0270  &        0.052     &    -0.517  &         0.608        &       -0.132    &        0.078     \\\\\n",
       "\\textbf{x5}    &       0.0270  &        0.017     &     1.574  &         0.123        &       -0.008    &        0.062     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 14.783 & \\textbf{  Durbin-Watson:     } &    1.283  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   21.267  \\\\\n",
       "\\textbf{Skew:}          & -0.948 & \\textbf{  Prob(JB):          } & 2.41e-05  \\\\\n",
       "\\textbf{Kurtosis:}      &  5.572 & \\textbf{  Cond. No.          } & 1.45e+06  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.45e+06. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Tue, 10 Oct 2023   Prob (F-statistic):           1.34e-27\n",
       "Time:                        20:34:13   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.855      7.281      0.000    3.63e+04     6.4e+04\n",
       "x1           198.7542   3371.026      0.059      0.953   -6595.103    6992.611\n",
       "x2           -42.0063   3256.058     -0.013      0.990   -6604.161    6520.148\n",
       "x3             0.8060      0.046     17.368      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.783   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.267\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./50_Startups.csv')\n",
    "X=data.iloc[:,:4].values\n",
    "Y=data.iloc[:,4].values\n",
    "\n",
    "\n",
    "\n",
    "#encoding the strings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "LabelEncoder_X = LabelEncoder()\n",
    "X[:,3]=LabelEncoder_X.fit_transform(X[:,3])\n",
    "oneHotEncoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_X=oneHotEncoder.fit_transform(X[:,3].reshape(-1,1))\n",
    "X=np.column_stack((encoded_X,X[:,:3]))\n",
    "\n",
    "# 3 dummy variable so we will take 2 in consideration getting rid of dummy  var trap\n",
    "\n",
    "X=X[:,1:]\n",
    "X\n",
    "#train and test our data\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "\n",
    "#linear regression\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# regressor = LinearRegression()\n",
    "# regressor.fit(X_train,Y_train)\n",
    "# y_pred = regressor.predict(X_test)\n",
    "\n",
    "# creating an accurate model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# in the equation y=b0x0+b1x1+.....+bnxn (x0=1) \n",
    "# so we have to add 1 in the whole X matrix\n",
    "\n",
    "array_of_once=np.ones((50,1)).astype(int)\n",
    "X=np.append(arr=array_of_once,values=X,axis=1)\n",
    "\n",
    "#Backward elimination\n",
    "\n",
    "# step-1 \n",
    "# set the siginificant level for example = 0.05\n",
    "\n",
    "# step-2\n",
    "#ALL-IN\n",
    "\n",
    "X_opt = np.array(X[:,[0,1,2,3,4,5]],dtype=int)\n",
    "\n",
    "regressor_OLS = sm.OLS(Y,X_opt).fit()\n",
    "regressor_OLS.summary()\n",
    "# X_opt = np.array(X[:,[0,1,3,4,5]],dtype=int)\n",
    "\n",
    "# regressor_OLS = sm.OLS(Y,X_opt).fit()\n",
    "# regressor_OLS.summary()\n",
    "# X_opt = np.array(X[:,[0,3,4,5]],dtype=int)\n",
    "\n",
    "# regressor_OLS = sm.OLS(Y,X_opt).fit()\n",
    "# regressor_OLS.summary()\n",
    "# X_opt = np.array(X[:,[0,3,5]],dtype=int)\n",
    "\n",
    "# regressor_OLS = sm.OLS(Y,X_opt).fit()\n",
    "# regressor_OLS.summary()\n",
    "# X_opt = np.array(X[:,[0,3]],dtype=int)\n",
    "\n",
    "regressor_OLS = sm.OLS(Y,X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
